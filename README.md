# TheLLM

Aim
- Flash Attention Inference
- RoPE encodings
- KV cache and Rolling buffer KV cache for streaming
- Sliding window attention and attention sink
- RMSNorm
- MoE
- SFT
- Reward Modeling
- RLHF with PPO or GRPO
- Have good evaluation benchmarks
- Inference Efficency (For later, QloRA and stuff)